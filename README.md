# 人工智能 Project 1

## 基础信息

**运行环境：**Python 3.12.1 

**IDE：**Pycharm 2024.1

**使用的第三方库：**（其中torch只在卷积神经网络任务中使用）

```
torch~=2.2.2
matplotlib~=3.8.4
scikit-learn~=1.4.2
numpy~=1.26.4
pillow~=10.3.0
```

**文件结构：**

**regression.py:** 回归任务的神经网络，训练过程

**classification.py:** 分类任务的神经网络以及读取，训练，验证过程

**cnn.py:** 卷积神经网络，以及训练验证的过程

## BP网

#### 回归任务

**实验结果：**经过100000次迭代，loss误差已经降至8*10^-5，经matplotlib绘制的拟合结果以及损失函数的下降趋势如下：![image-20240419225120619](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20240419225120619.png)

![image-20240419225127256](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20240419225127256.png)

**设计思路**：

首先设计了 SimpleNeuralNetwork这个类，选择了relu这个激活函数，以及均方差的loss函数，He初始化方法（何凯明提出的针对relu的优化策略），由于难度较低，在第一个任务中没有使用其他优化方法。

这个网络架构是可以伸缩调整的，只需要在传入初始化参数的时候提供一个list即可，如[1,10,1]代表一个输入层，含一个神经元，一个隐藏层，含10个神经元，一个输出层，含一个神经元。前向传播过程中，通过线性变换，激活函数，并保存激活结果（最后一层不激活）。在反向传播过程也是同理，使用梯度下降法，更新权重和偏置。

总体来讲这个任务比较简单，很容易收敛误差到极小值。

#### 分类任务

![image-20240420212454271](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20240420212454271.png)

**实验结果：**

Epoch 190, Loss: 0.8214658696375163, Accuracy: 0.8897849462365591

loss函数在弃用dropout和正则化的情况下可以收敛到非常接近0，但此时的准确度反而减小，因此选择了看似loss函数大，但是准确度高的模型。

**设计思路：**

分类任务的类基本继承了回归任务的类，但在这之上加入了许多新的内容。

对于训练内容的读取，用img.resize(img_size).convert('L') 来调整图片大小并将其转换为灰度图。转换为灰度图是因为卷积神经网络在处理单通道图像（如文本或灰度图像）时计算量更小。将图片转换为Numpy数组，并除以255.0实现归一化，可以帮助加速模型训练，并提高模型的收敛速度。

然后使用了sklearn这个库中的函数，将助教提供的620*12张图片进行处理，80%为训练集，剩余的20%为测试集，因此这里的accuracy具有很强的代表价值。

其次是设计了adam优化器，adam自动调整每个参数的学习率，基于过去梯度的一阶矩（均值）和二阶矩（未中心化的方差）估计。这种方法有助于处理稀疏梯度或不同参数的不同方差，特别是在复杂的模型和大规模数据集上。

剩余还有很多防止过拟合的方法，在本文最后的bonus部分有详细介绍。

这个任务相对较为困难，loss函数很容易收敛到极小值，但是正确率的提高非常困难，甚至loss函数的降低并不代表正确率的升高。

## 卷积神经网络

**实验结果：**

![image-20240419230924718](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20240419230924718.png)

在Epoch 90,  Loss:0.0017899803971240839，Accuracy:100.0%，在随机划分的训练集上取得了100%的正确率

**设计思路：**

对于训练集和测试集的读取，使用了前一个任务的函数，这里主要介绍该卷积神经网络的特点。

conv1卷积层：这是第一个卷积层。它接受单通道输入（例如灰度图像），应用32个滤波器（或称为内核），每个大小为3x3，填充为1。填充1的作用是使使用3x3内核时输出的空间维度与输入相同。
conv2卷积层：第二个卷积层接受conv1的32个特征图，应用64个同样大小（3x3）的滤波器，同样的填充为1。
pool池化层：一个最大池化层，窗口为2x2，步长为2。此层通过减半特征图的空间维度来降采样数据。
fc1全连接层：一个线性（或全连接）层，将最后一个池化层输出的展平特征图转换为128维空间。
fc2全连接层：另一个线性层，将128维向量映射到12个类别。

这个任务相比用bp网反而简单很多，非常容易达到极高的准确率，可见卷积神经网络在图片分类问题上的强大。

## BONUS：减少过拟合的方法

**正则化：**

这里我实现的是L2正则化，但因为效果不是特别理想，在最终训练中并没有使用。通过在损失函数中添加一个与权重的平方成比例的惩罚项来工作。这个惩罚项会促使模型学习到更小的权重值，从而减少模型的复杂度。

**Dropout：**

在训练过程中，Dropout随机地丢弃（即将输出置为0）一部分网络节点（包括隐藏层和可视层），每个节点被丢弃的概率是独立的。这种随机性的引入使得模型不能依赖于任何一个节点，能够非常显著提高防止过拟合的能力。这个方法的用处非常大。

**早停：**

当模型在验证集上的表现连续几个训练周期不再提升或者开始变差时，我们停止训练。

**Batch:**

使用批量可以减少训练过程中的内存消耗，因为整个数据集不需要一次性加载到内存中。此外，批量处理还可以帮助网络更平滑地学习，降低训练过程中的方差，提升模型的稳定性。实际上batch也是一种正则化方式。
